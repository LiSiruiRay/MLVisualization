{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-28T08:38:53.373475Z",
     "start_time": "2024-05-28T08:38:53.258702Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasetsforecast.long_horizon import LongHorizon\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "\n",
    "\n",
    "# Change this to your own data to try the model\n",
    "Y_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "n_time = len(Y_df.ds.unique())\n",
    "val_size = int(.2 * n_time)\n",
    "test_size = int(.2 * n_time)\n",
    "\n",
    "Y_df.groupby('unique_id').head(2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       unique_id                  ds         y\n",
       "0           HUFL 2016-07-01 00:00:00 -0.041413\n",
       "1           HUFL 2016-07-01 00:15:00 -0.185467\n",
       "57600       HULL 2016-07-01 00:00:00  0.040104\n",
       "57601       HULL 2016-07-01 00:15:00 -0.214450\n",
       "115200      LUFL 2016-07-01 00:00:00  0.695804\n",
       "115201      LUFL 2016-07-01 00:15:00  0.434685\n",
       "172800      LULL 2016-07-01 00:00:00  0.434430\n",
       "172801      LULL 2016-07-01 00:15:00  0.428168\n",
       "230400      MUFL 2016-07-01 00:00:00 -0.599211\n",
       "230401      MUFL 2016-07-01 00:15:00 -0.658068\n",
       "288000      MULL 2016-07-01 00:00:00 -0.393536\n",
       "288001      MULL 2016-07-01 00:15:00 -0.659338\n",
       "345600        OT 2016-07-01 00:00:00  1.018032\n",
       "345601        OT 2016-07-01 00:15:00  0.980124"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>-0.041413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>-0.185467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57600</th>\n",
       "      <td>HULL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>0.040104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57601</th>\n",
       "      <td>HULL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>-0.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115200</th>\n",
       "      <td>LUFL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>0.695804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115201</th>\n",
       "      <td>LUFL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>0.434685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172800</th>\n",
       "      <td>LULL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>0.434430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172801</th>\n",
       "      <td>LULL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>0.428168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230400</th>\n",
       "      <td>MUFL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>-0.599211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230401</th>\n",
       "      <td>MUFL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>-0.658068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288000</th>\n",
       "      <td>MULL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>-0.393536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288001</th>\n",
       "      <td>MULL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>-0.659338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345600</th>\n",
       "      <td>OT</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>1.018032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345601</th>\n",
       "      <td>OT</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>0.980124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T08:39:01.125198Z",
     "start_time": "2024-05-28T08:39:01.094204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import Autoformer\n",
    "horizon = 96 # 24hrs = 4 * 15 min.\n",
    "models = [#Informer(h=horizon,                 # Forecasting horizon\n",
    "                # input_size=horizon,           # Input size\n",
    "                # max_steps=1000,               # Number of training iterations\n",
    "                # val_check_steps=100,          # Compute validation loss every 100 steps\n",
    "                # early_stop_patience_steps=3), # Stop training if validation loss does not improve\n",
    "          Autoformer(h=horizon,\n",
    "                input_size=horizon,\n",
    "                max_steps=1000,\n",
    "                val_check_steps=100,\n",
    "                early_stop_patience_steps=3),\n",
    "          # PatchTST(h=horizon,\n",
    "          #       input_size=horizon,\n",
    "          #       max_steps=1000,\n",
    "          #       val_check_steps=100,\n",
    "          #       early_stop_patience_steps=3),\n",
    "         ]\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=models,\n",
    "    freq='15min')"
   ],
   "id": "c5fdccfbb319fbbd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T08:39:07.684408Z",
     "start_time": "2024-05-28T08:39:03.991102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "Y_hat_df = nf.cross_validation(df=Y_df,\n",
    "                               val_size=val_size,\n",
    "                               test_size=test_size,\n",
    "                               n_windows=None)"
   ],
   "id": "d539f0de5122f5e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (mps), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | loss          | MAE           | 0     \n",
      "1 | padder_train  | ConstantPad1d | 0     \n",
      "2 | scaler        | TemporalNorm  | 0     \n",
      "3 | decomp        | SeriesDecomp  | 0     \n",
      "4 | enc_embedding | DataEmbedding | 384   \n",
      "5 | dec_embedding | DataEmbedding | 384   \n",
      "6 | encoder       | Encoder       | 148 K \n",
      "7 | decoder       | Decoder       | 141 K \n",
      "------------------------------------------------\n",
      "290 K     Trainable params\n",
      "0         Non-trainable params\n",
      "290 K     Total params\n",
      "1.162     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d10d51df11748249bf64e551704cbe0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_fft_r2c' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      2\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPYTORCH_ENABLE_MPS_FALLBACK\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 4\u001B[0m Y_hat_df \u001B[38;5;241m=\u001B[39m \u001B[43mnf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mY_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mval_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mn_windows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/core.py:1011\u001B[0m, in \u001B[0;36mNeuralForecast.cross_validation\u001B[0;34m(self, df, static_df, n_windows, step_size, val_size, test_size, sort_df, use_init_models, verbose, refit, id_col, time_col, target_col, **data_kwargs)\u001B[0m\n\u001B[1;32m   1009\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mreset_index(id_col)\n\u001B[1;32m   1010\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m refit:\n\u001B[0;32m-> 1011\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_refit_cross_validation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1013\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstatic_df\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstatic_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1014\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_windows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_windows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstep_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstep_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[43m        \u001B[49m\u001B[43mval_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort_df\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m        \u001B[49m\u001B[43mid_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mid_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtime_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1026\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMust specify `df` with `refit!=False`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/core.py:874\u001B[0m, in \u001B[0;36mNeuralForecast._no_refit_cross_validation\u001B[0;34m(self, df, static_df, n_windows, step_size, val_size, test_size, sort_df, verbose, id_col, time_col, target_col, **data_kwargs)\u001B[0m\n\u001B[1;32m    867\u001B[0m fcsts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfull(\n\u001B[1;32m    868\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mn_groups \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh \u001B[38;5;241m*\u001B[39m n_windows, \u001B[38;5;28mlen\u001B[39m(cols)),\n\u001B[1;32m    869\u001B[0m     np\u001B[38;5;241m.\u001B[39mnan,\n\u001B[1;32m    870\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[1;32m    871\u001B[0m )\n\u001B[1;32m    873\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels:\n\u001B[0;32m--> 874\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    875\u001B[0m     model_fcsts \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(\n\u001B[1;32m    876\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, step_size\u001B[38;5;241m=\u001B[39mstep_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata_kwargs\n\u001B[1;32m    877\u001B[0m     )\n\u001B[1;32m    879\u001B[0m     \u001B[38;5;66;03m# Append predictions in memory placeholder\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/common/_base_windows.py:651\u001B[0m, in \u001B[0;36mBaseWindows.fit\u001B[0;34m(self, dataset, val_size, test_size, random_seed, distributed_config)\u001B[0m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    624\u001B[0m     dataset,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    628\u001B[0m     distributed_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    629\u001B[0m ):\n\u001B[1;32m    630\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit.\u001B[39;00m\n\u001B[1;32m    631\u001B[0m \n\u001B[1;32m    632\u001B[0m \u001B[38;5;124;03m    The `fit` method, optimizes the neural network's weights using the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    649\u001B[0m \u001B[38;5;124;03m    `test_size`: int, test size for temporal cross-validation.<br>\u001B[39;00m\n\u001B[1;32m    650\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    653\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    654\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalid_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalid_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    655\u001B[0m \u001B[43m        \u001B[49m\u001B[43mval_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    656\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    657\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_seed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    658\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdistributed_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdistributed_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    659\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/common/_base_model.py:219\u001B[0m, in \u001B[0;36mBaseModel._fit\u001B[0;34m(self, dataset, batch_size, valid_batch_size, val_size, test_size, random_seed, shuffle_train, distributed_config)\u001B[0m\n\u001B[1;32m    217\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m    218\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrainer_kwargs)\n\u001B[0;32m--> 219\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatamodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m model\u001B[38;5;241m.\u001B[39mmetrics \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mcallback_metrics\n\u001B[1;32m    221\u001B[0m model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_trainer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    576\u001B[0m     ckpt_path,\n\u001B[1;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    579\u001B[0m )\n\u001B[0;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 987\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    990\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    992\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1031\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m   1033\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1057\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m-> 1060\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1062\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[1;32m    390\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    391\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[1;32m    395\u001B[0m )\n\u001B[0;32m--> 396\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 309\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    312\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:412\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/common/_base_windows.py:528\u001B[0m, in \u001B[0;36mBaseWindows.validation_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m    519\u001B[0m windows_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    520\u001B[0m     insample_y\u001B[38;5;241m=\u001B[39minsample_y,  \u001B[38;5;66;03m# [Ws, L]\u001B[39;00m\n\u001B[1;32m    521\u001B[0m     insample_mask\u001B[38;5;241m=\u001B[39minsample_mask,  \u001B[38;5;66;03m# [Ws, L]\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    524\u001B[0m     stat_exog\u001B[38;5;241m=\u001B[39mstat_exog,\n\u001B[1;32m    525\u001B[0m )  \u001B[38;5;66;03m# [Ws, 1]\u001B[39;00m\n\u001B[1;32m    527\u001B[0m \u001B[38;5;66;03m# Model Predictions\u001B[39;00m\n\u001B[0;32m--> 528\u001B[0m output_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mwindows_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    529\u001B[0m valid_loss_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_valid_loss(\n\u001B[1;32m    530\u001B[0m     outsample_y\u001B[38;5;241m=\u001B[39moriginal_outsample_y,\n\u001B[1;32m    531\u001B[0m     output\u001B[38;5;241m=\u001B[39moutput_batch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    534\u001B[0m     y_idx\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    535\u001B[0m )\n\u001B[1;32m    536\u001B[0m valid_losses\u001B[38;5;241m.\u001B[39mappend(valid_loss_batch)\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/models/autoformer.py:693\u001B[0m, in \u001B[0;36mAutoformer.forward\u001B[0;34m(self, windows_batch)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;66;03m# enc\u001B[39;00m\n\u001B[1;32m    692\u001B[0m enc_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menc_embedding(insample_y, x_mark_enc)\n\u001B[0;32m--> 693\u001B[0m enc_out, attns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43menc_out\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;66;03m# dec\u001B[39;00m\n\u001B[1;32m    695\u001B[0m dec_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdec_embedding(seasonal_init, x_mark_dec)\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/models/autoformer.py:329\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, x, attn_mask)\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attn_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_layers:\n\u001B[0;32m--> 329\u001B[0m         x, attn \u001B[38;5;241m=\u001B[39m \u001B[43mattn_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    330\u001B[0m         attns\u001B[38;5;241m.\u001B[39mappend(attn)\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/models/autoformer.py:295\u001B[0m, in \u001B[0;36mEncoderLayer.forward\u001B[0;34m(self, x, attn_mask)\u001B[0m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, attn_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 295\u001B[0m     new_x, attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(new_x)\n\u001B[1;32m    297\u001B[0m     x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecomp1(x)\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/models/autoformer.py:204\u001B[0m, in \u001B[0;36mAutoCorrelationLayer.forward\u001B[0;34m(self, queries, keys, values, attn_mask)\u001B[0m\n\u001B[1;32m    201\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_projection(keys)\u001B[38;5;241m.\u001B[39mview(B, S, H, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    202\u001B[0m values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_projection(values)\u001B[38;5;241m.\u001B[39mview(B, S, H, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 204\u001B[0m out, attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner_correlation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mview(B, L, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_projection(out), attn\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/lee_ml_class/lib/python3.10/site-packages/neuralforecast/models/autoformer.py:160\u001B[0m, in \u001B[0;36mAutoCorrelation.forward\u001B[0;34m(self, queries, keys, values, attn_mask)\u001B[0m\n\u001B[1;32m    157\u001B[0m     keys \u001B[38;5;241m=\u001B[39m keys[:, :L, :, :]\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m# period-based dependencies\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m q_fft \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfft\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrfft\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqueries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m k_fft \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfft\u001B[38;5;241m.\u001B[39mrfft(keys\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous(), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    162\u001B[0m res \u001B[38;5;241m=\u001B[39m q_fft \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mconj(k_fft)\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: The operator 'aten::_fft_r2c' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from neuralforecast.losses.numpy import mae\n",
    "\n",
    "mae_autoformer = mae(Y_hat_df['y'], Y_hat_df['Autoformer'])\n",
    "print(f'Autoformer: {mae_autoformer:.3f}')"
   ],
   "id": "25c3085940430c66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
